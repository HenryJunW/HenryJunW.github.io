
<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Jun Wang</title>
  
  <meta name="author" content="Jon Barron">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Jun Wang</name>
              </p>
              <p>Jun is a Senior Machine Learning Engineer at <a href=https://www.salesforceairesearch.com>Salesforce Research</a>, working on <strong> multimodal LLM</strong>. He earned his Ph.D. in Electrical and Computer Engineering at <a href="https://ece.umd.edu/">University of Maryland, College Park</a>, where he was honored to be co-advised by Prof. <a href="https://lsd.umiacs.io"target="_blank">Larry S. Davis</a> and Prof. <a href="http://users.umiacs.umd.edu/~joseph/"target="_blank">Joseph F. JaJa</a>. His research primarily focuses on <strong> multi-modal learning</strong>, <strong> object detection</strong>, and <strong> 3D scene understanding</strong>. Currently, he is on the job market.</p>
              <br>
              <!-- Jun has in-depth knowledge and hands-on experience in the large-scale datasets for <strong> autonomous driving</strong>, e.g., KITTI, Waymo, nuScenes, SemanticKITTI and Argoverse. Also, he has multiple years of industry/teaching experience in integrated circuit design using RTL and high level synthesis tools.  -->
              Recently, Jun has been fortunate to work with Dr. <a href="https://scholar.google.com/citations?user=3-8i_54AAAAJ&hl=en"> Kishore Prahallad</a> (Apple), Dr. <a href="https://scholar.google.com/citations?user=kMe-G5AAAAAJ&hl=en"> Mingfei Gao</a> and Dr. <a href="https://scholar.google.com/citations?user=sgBB2sUAAAAJ&hl=en"> Ran Xu</a> (Salesforce Research), and Dr. <a href="https://scholar.google.com/citations?user=W_Q33RMAAAAJ&hl=en"> Siheng Chen</a> (Mitsubishi Electric Research Labs). Prior to that, he obtained his M.Sc. degree in Electrical and Computer Engineering from <a href="https://eecs.engin.umich.edu">University of Michigan, Ann Arbor</a> in 2017 and B.Eng. degree from <a href="https://sie.bit.edu.cn/">Beijing Institute of Technology, China</a> in 2015.
              </p>
              <p style="text-align:center">
                Email: junwong [AT] terpmail [DOT] umd [DOT] edu
              </p>
              <p style="text-align:center">
                <!-- <a href="https://junwang.umiacs.io/">CV</a> -->
                <!-- &nbsp / &nbsp -->
                <a href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AOAOcb3ZAMggHIYf7aDyklN49-stjvbEETX3s9RhLwBrrstakvT682falI8zWAM5z6oTz-vQeHAMXzahXLAKpA&user=hf0bQ8AAAAAJ">Google Scholar</a>
                <!-- <a href="https://scholar.google.com/citations?user=vaN3jJ4AAAAJ&hl=en">Google Scholar</a> -->
                &nbsp / &nbsp
                <a href="https://www.semanticscholar.org/author/Jun-Wang/2152812028">Semantic Scholar</a>
                &nbsp / &nbsp
                <a href="https://dblp.dagstuhl.de/pid/125/8189-90.html">DBLP</a>
                &nbsp / &nbsp
                <a href="https://www.linkedin.com/in/wongjun/">LinkedIn</a>
                &nbsp / &nbsp
                <a href="https://github.com/HenryJunW">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="files/Jun_WANG.JPG"><img style="width:100%;max-width:100%" alt="profile photo" src="files/Jun_WANG.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <ul>
            <li><span> [Dec. 2024][<font color="red"><strong>New</strong></font>] <a href=https://huggingface.co/datasets/Salesforce/ProVision-10M>ProVision</a> is now released. Excited to contribute to our open-source instruction data generation pipeline to train multimodal LLM.
            <li><span> [Aug. 2024] <a href=https://arxiv.org/abs/2408.08872>xGen-MM (BLIP-3)</a> model is officialy <a href=https://huggingface.co/collections/Salesforce/xgen-mm-1-models-662971d6cecbf3a7f80ecc2e>released</a>. Thrilled to contribute to advancing open-source multimodal LLM. Don't miss our BLIP3-tailored datasets: <a href=https://huggingface.co/datasets/Salesforce/blip3-ocr-200m>BLIP3-OCR-200M</a> and <a href=https://huggingface.co/datasets/Salesforce/blip3-grounding-50m>BLIP3-Grounding-50M</a>—check them out!
            <li><span> [Aug. 2024] <a href=https://arxiv.org/abs/2408.12590>xGen-VideoSyn-1</a> is now available! Excited to contribute to our open-source video generation model. Stay tuned for the upcoming release!
            <li><span> [Feb. 2024] Start working as a senior machine learning engineer on large scale multi-modal learning and generative AI at <a href="https://www.salesforceairesearch.com"> Salesforce Research</a>, Palo Alto, CA.
            <li><span> [Dec 2023] Two patent applications for scene flow estimation in autonomous driving are filed.
            <li><span> [July 2023] Start working as a senior machine learning engineer on automated driving at <a href="https://www.qualcomm.com/products/automotive/overview/"> Qualcomm</a>, Novi, MI.
            <li><span> [Mar. 2023] Their work <a href=https://boheumd.github.io/A2Summ/>A2Summ</a>, on multi-modal summarization is accepted by CVPR 2023. Hello Vancouver!
            <li><span> [Sept. 2022] Their work <a href=https://bmvc2022.mpi-inf.mpg.de/33/>TAG</a>, a generic text-aware question-answer generation approach for Text-related VQA is accepted by BMVC 2022.
            <li><span> [Aug. 2022] Their work <a href=https://link.springer.com/chapter/10.1007/978-3-031-25066-8_41>NAPL</a>, a novel prototype learning paradigm for 3D LiDAR point cloud semantic segmentation will be presented at <a href=https://sites.google.com/view/cv4metaverse/home>Computer Vision for Metaverse Workshop</a>, ECCV 2022. 
            <li><span> [June 2022] The work <a href=https://www.isca-speech.org/archive/interspeech_2022/wang22n_interspeech.html>ESSumm</a>, an unsupervised speech summarization framework employing Wav2Vec, is accepted by INTERSPEECH 2022. Annyeong haseyo, Incheon.
            <li><span> [Apr. 2022] A <a href=https://patents.google.com/patent/US20230342944A1/en>patent</a> application for motion prediction in autonomous driving is filed.
            <li><span> [Apr. 2022] Their paper "<a href=https://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.html>PointMotionNet: Point-Wise Motion Learning for Large-Scale LiDAR Point Clouds Sequences</a>" will be presented at <a href=https://cvpr2022.wad.vision>Workshop on Autonomous Driving (WAD)</a>, CVPR 2022. Where y'at, New Orleans.
            <li><span> [Jan. 2022] The <a href=https://github.com/rayguan97/M3DETR>code</a> for M3DETR is released. He gave a presentation of M3DETR at WACV 2022, Waikoloa, Hawaii.
            <li><span> [Nov. 2021] Their work <a href=https://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.html>PointMotionNet</a>, a framework of 3D motion learning on LiDAR point clouds, achieves the 4th place out of 85 teams in the leaderboard of <a href=https://competitions.codalab.org/competitions/20331#results>SemanticKITTI Multiscan Semantic Segmentation</a>.
            <li><span> [Aug. 2021] Pass his Ph.D. research proposal examination and advance to candidacy.
            <li><span> [July 2021] Their Paper "<a href=https://arxiv.org/abs/2104.11896>M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers</a>" is accepted by WACV 2022 in the First Round. Aloha, Hawaii.</li>
            <li><span> [May 2021] Start his machine learning internship with Dr. <a href="https://scholar.google.com/citations?user=3-8i_54AAAAJ&hl=en"> Kishore Prahallad</a> at <a href="https://machinelearning.apple.com"> Apple AI/ML</a>, Cupertino, CA.
            <li><span> [Feb. 2021] Start his research internship with Dr. <a href="https://scholar.google.com/citations?user=kMe-G5AAAAAJ&hl=en"> Mingfei Gao</a> and Dr. <a href="https://scholar.google.com/citations?user=sgBB2sUAAAAJ&hl=en"> Ran Xu</a>  at <a href="https://www.salesforceairesearch.com"> Salesforce Research</a>, Palo Alto, CA.    
            <li><span> [Nov. 2020] A manuscript on 3D motion learning in LiDAR point clouds is under review. Fingers Crossed.
            <li><span> [Sept. 2020] Start his research internship with Prof. <a href="https://scholar.google.com/citations?user=W_Q33RMAAAAJ&hl=en"> Siheng Chen</a> (now Shanghai Jiao Tong University) at <a href="https://www.merl.com/"> Mitsubishi Electric Research Labs</a>, Cambridge, MA.
            <li><span> [July 2020] Their Paper "<a href=https://arxiv.org/abs/2007.08556>InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling</a>" is accepted by ECCV 2020.</li>
        </ul>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/BLIP3.png' width="160">
            </div>
          </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.salesforceairesearch.com/opensource/xGen-MM/index.html">
              <papertitle>xGen-MM (BLIP-3): A Family of Open Large Multimodal Models</papertitle>
            </a>
            <br>
            Le Xue*, Manli Shu*, Anas Awadalla, <strong>Jun Wang</strong>, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu, Yutong Dai, Michael S Ryoo, Shrikant Kendre, Jieyu Zhang, Can Qin, Shu Zhang, Chia-Chih Chen, Ning Yu, Juntao Tan, Tulika Manoj Awalgaonkar, Shelby Heinecke, Huan Wang, Yejin Choi, Ludwig Schmidt, Zeyuan Chen, Silvio Savarese, Juan Carlos Niebles, Caiming Xiong, Ran Xu
            <br>
            <em>arXiv</em>, 2024
            <br>          
            <a href="https://arxiv.org/abs/2408.08872">arXiv</a>
            /
            <a href="https://huggingface.co/Salesforce/xgen-mm-phi3-mini-instruct-interleave-r-v1.5">code</a>
            /
            <a href="https://huggingface.co/collections/Salesforce/xgen-mm-1-models-662971d6cecbf3a7f80ecc2e">dataset</a>
            /
            <a href="https://venturebeat.com/ai/salesforce-releases-xgen-mm-open-source-multimodal-ai-models-to-advance-visual-language-understanding/">press coverage</a>
            <p></p>
            <p>Open-sourced multimodal Large Language Models (MLLM). </p>
          </td>
        </tr>

      <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/ProVision.png' width="160">
            </div>
          </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.salesforce.com/blog/provision-multimodal-data-generation/">
              <papertitle>ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models</papertitle>
            </a>
            <br>
            Jieyu Zhang, Le Xue, Linxin Song, <strong>Jun Wang</strong>, Weikai Huang, Manli Shu, An Yan, Zixian Ma, Juan Carlos Niebles, Silvio Savarese, Caiming Xiong, Zeyuan Chen, Ranjay Krishna, Ran Xu
            <br>
            <em>arXiv</em>, 2024
            <br>          
            <a href="https://arxiv.org/abs/2412.07012">arXiv</a>
            /
            <a href="https://github.com/JieyuZ2/ProVision">code</a>
            /
            <a href="https://huggingface.co/datasets/Salesforce/ProVision-10M">dataset</a>
            /
            <a href="https://venturebeat.com/data-infrastructure/breaking-the-data-bottleneck-salesforces-provision-speeds-multimodal-ai-training-with-image-scene-graphs/">press coverage</a>
            <p></p>
            <p>A scalable system generating 10M+ vision-centric
            instructions, improving multimodal benchmark by 8%. </p>
          </td>
        </tr>

<tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/xGen-VideoSyn.png' width="160">
            </div>
          </td>

      <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2408.12590">
              <papertitle>xGen-VideoSyn-1: High-fidelity Text-to-Video Synthesis with Compressed Representations</papertitle>
            </a>
            <br>
            Can Qin, Congying Xia, Krithika Ramakrishnan, Michael Ryoo, Lifu Tu, Yihao Feng, Manli Shu, Honglu Zhou, Anas Awadalla, <strong>Jun Wang</strong>, Senthil Purushwalkam, Le Xue, Yingbo Zhou, Huan Wang, Silvio Savarese, Juan Carlos Niebles, Zeyuan Chen, Ran Xu, Caiming Xiong
            <br>
            <em>arXiv</em>, 2024
            <br>          
            <a href="https://arxiv.org/abs/2408.12590">arXiv</a>
            /
            <a href="https://github.com/SalesforceAIResearch/xgen-videosyn">code</a>
            <p></p>
            <p>A T2V model leveraging VideoVAE compression and Diffusion Transformer. </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/A2Summ.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/html/He_Align_and_Attend_Multimodal_Summarization_With_Dual_Contrastive_Losses_CVPR_2023_paper.html">
              <papertitle>Align and Attend: Multimodal Summarization with Dual Contrastive Losses</papertitle>
            </a>
            <br>
            <a href="https://boheumd.github.io"> Bo He</a>, 
            <strong>Jun Wang</strong>,
            <a href="https://www.cs.cmu.edu/~jielinq/"> Jielin Qiu</a>, 
            <a href="https://sites.google.com/site/trungbuistanford/"> Trung Bui</a>, 
            <a href="https://www.cs.umd.edu/~abhinav/"> Abhinav Shrivastava</a>, 
            <a href="https://scholar.google.com/citations?user=lwlYARMAAAAJ&hl=en"> Zhaowen Wang</a> 
            <br>
            <em>CVPR</em>, 2023
            <br>          
            <a href="https://arxiv.org/abs/2303.07284">arXiv</a>
            /
            <a href="https://github.com/boheumd/A2Summ">code</a>
            /
            <a href="https://boheumd.github.io/A2Summ/">project</a>
            /
            <a href="bibtex/A2Summ.txt" target="_blank">bibtex</a>
            <p></p>
            <p>Multimodal summarization that summarizes video frames and text sentences with time correspondence. </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/TAG_overview_figure.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://bmvc2022.mpi-inf.mpg.de/33/">
              <papertitle>TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation</papertitle>
            </a>
            <br>
            <strong>Jun Wang</strong>,
            <a href="https://scholar.google.com/citations?user=kMe-G5AAAAAJ&hl=en"> Mingfei Gao</a>, 
            <a href="https://yuqianhu09.github.io"> Yuqian Hu</a>, 
            <a href="https://ramprs.github.io"> Ramprasaath R. Selvaraju</a>, 
            <a href="https://scholar.google.com/citations?user=gIqrE4cAAAAJ&hl=en"> Chetan Ramaiah</a>, 
            <a href="https://scholar.google.com/citations?user=sgBB2sUAAAAJ&hl=en"> Ran Xu</a>, 
            <a href="http://users.umiacs.umd.edu/~joseph/"> Joseph F. JaJa</a>,  
            <a href="https://lsd.umiacs.io"> Larry S. Davis</a>
            <br>
            <em>BMVC</em>, 2022
            <br>          
            <a href="https://arxiv.org/abs/2208.01813">arXiv</a>
            /
            <a href="https://github.com/HenryJunW/TAG">code</a>
            /
            <a href="https://bmvc2022.mpi-inf.mpg.de/0033_poster.pdf">poster</a>
            /
            <a href="bibtex/TAG.txt" target="_blank">bibtex</a>
            <p></p>
            <p>The first generic text-aware question-answer generation approach for Text-related VQA.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/Architecture_ESSsumm.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.isca-speech.org/archive/interspeech_2022/wang22n_interspeech.html">
              <papertitle>ESSumm: Extractive Speech Summarization from Untranscribed Meeting</papertitle>
            </a>
            <br>
            <strong>Jun Wang</strong>
            <br>
            <em>INTERSPEECH</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2209.06913">arXiv</a>
            /
            <a href="https://github.com/HenryJunW/ESSumm">code</a>
            /
            <a href="https://drive.google.com/file/d/1p9JaYLFk5OnSGuMXOQ7ADFOCmCOZD78b/view?usp=sharing">slides</a>
            /
            <a href="bibtex/ESSumm.txt" target="_blank">bibtex</a>
            <p></p>
            <p>The first automatic speech summarization system with Wav2vec 2.0.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/PointMotionNet.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.html">
              <papertitle>PointMotionNet: Point-Wise Motion Learning for Large-Scale LiDAR Point Clouds Sequences</papertitle>
            </a>
            <br>
            <strong> Jun Wang*</strong>, 
            <a href="https://dragonlong.github.io"> Xiaolong Li*</a>,
            <a href="https://scholar.google.com/citations?user=QrGE5bMAAAAJ&hl=en">Alan Sullivan</a>,
            <a href="https://scholar.google.com/citations?user=UJVd84YAAAAJ&hl=en">Lynn Abbott</a>,
            <a href="https://scholar.google.com/citations?user=W_Q33RMAAAAJ&hl=en">Siheng Chen</a>
            <br>
            * denotes equal contribution.
            <br>
            <em>WAD, CVPR</em>, 2022
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2022W/WAD/html/Wang_PointMotionNet_Point-Wise_Motion_Learning_for_Large-Scale_LiDAR_Point_Clouds_Sequences_CVPRW_2022_paper.html">arXiv</a>
            /
            <a href="bibtex/PointMotionNet.txt" target="_blank">bibtex</a>
            <p></p>
            <p>3D motion learning with a novel point-based spatiotemporal convolution operation module. </p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/M3DETR_arch.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://openaccess.thecvf.com/content/WACV2022/html/Guan_M3DETR_Multi-Representation_Multi-Scale_Mutual-Relation_3D_Object_Detection_With_Transformers_WACV_2022_paper.html">
              <papertitle>M3DETR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers</papertitle>
            </a>
            <br>
            <strong>Jun Wang*</strong>,
            <a href="https://rayguan97.github.io/">Tianrui Guan*</a>,
            <a href="https://voidrank.github.io/"> Shiyi Lan</a>,
            <a href="https://rohanchandra30.github.io/">Rohan Chandra</a>,
            <a href="http://zxwu.azurewebsites.net/">Zuxuan Wu</a>,
            <a href="https://lsd.umiacs.io">Larry S. Davis</a>,
            <a href="https://www.cs.umd.edu/people/dmanocha">Dinesh Manocha</a>
            <br>
            * denotes equal contribution.
            <br>
            <em>WACV</em>, 2022
            <br>
            <a href="https://arxiv.org/abs/2104.11896">arXiv</a>
            /
            <a href="https://github.com/rayguan97/M3DeTR">code</a>
            /
            <a href="https://drive.google.com/file/d/1CfDlBk2oy5FEUdBtDlNB5lFdajiHvJg1/view?usp=sharing">slides</a> 
            /
            <a href="bibtex/M3DETR.txt" target="_blank">bibtex</a>
            <p></p>
            <p>The multi-representation, multi-scale, mutual-relation 3D object detector with transformers.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/Infofocus.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/1137_ECCV_2020_paper.php">
              <papertitle>InfoFocus: 3D Object Detection for Autonomous Driving with Dynamic Information Modeling</papertitle>
            </a>
            <br>
            <strong> Jun Wang*</strong>, 
            <a href="https://voidrank.github.io/"> Shiyi Lan*</a>,
            <a href="https://scholar.google.com/citations?user=kMe-G5AAAAAJ&hl=en">Mingfei Gao</a>,
            <a href="https://lsd.umiacs.io">Larry S. Davis</a>
            <br>
            * denotes equal contribution.
            <br>
            <em>ECCV</em>, 2020
            <br>
            <a href="https://arxiv.org/abs/2007.08556">arXiv</a>
            /
            <a href="https://drive.google.com/file/d/13uDHuZOFWNcEfnQRGiI8THm0sXD8GteL/view?usp=sharing">slides</a> 
            /
            <a href="bibtex/InfoFocus.txt" target="_blank">bibtex</a>
            <p></p>
            <p>3D Object Detection with the effective dynamic attention module. </p>
          </td>
        </tr>
        </tbody></table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Misc Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/470_diagram.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>RTL Design of R10K Out-of-Order 2-way Superscalar Processor with Simultaneous Multithreading</papertitle>
            </a>
            <br>
            <a href="https://www.linkedin.com/in/ruobai-feng-4635a9114"> Ruobai Feng</a>, 
            <a href="https://www.linkedin.com/in/eewangcao"> Wang Cao</a>, 
            <strong>Jun Wang</strong>,
            <a href="https://sites.google.com/umich.edu/yujunyan/home"> Yujun Yan</a>, 
            <a>Jiapeng Zhao</a>
            <br>
            <em>EECS 470 Computer Architecture</em>, 2016
            <br>          
            <p></p>
            <p>The two-way superscalar SMT processor design based on MIPS R10K out- of-order execution architecture.</p>
          </td>
        </tr>


        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='teasers/427_layout.png' width="160">
            </div>
          </td>

          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Design and Layout of a 16-bit RISC Pipelined Processor</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=t6NFe50AAAAJ&hl=en"> Farzad Asgarian</a>, 
            <a href="https://www.linkedin.com/in/harsha-chawla-3a528662"> Harsha Chawla</a>, 
            <a href="https://www.linkedin.com/in/isaac-jarman-9a929544"> Isaac Jarman</a>, 
            <a href="https://www.linkedin.com/in/cody-piekarz-aa814465"> Cody Piekarz</a>, 
            <strong>Jun Wang</strong>
            <br>
            <em>EECS 427 VLSI Design I</em>, 2015
            <br>          
            <p></p>
            <p>The baseline processor design with a customized kogge-stone adder based on a 16-bit RISC architecture using IBM’s 130nm CMOS process.</p>
          </td>
        </tr>


        </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <ul>
            <li><span> Program Committee: AAAI'25</li>
            <li><span> Conference Reviewer: CVPR, ICCV, ECCV, ICML, NeurIPS, ICLR, AAAI, BMVC, WACV, ACM MM</li>
            <li><span> Journal Reviewer: Neurocomputing, IEEE Transactions on Circuits and Systems for Video Technology</li>
            <li><span> Student Volunteer: INTERSPEECH'22</li>
        </ul>
   </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Awards</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <ul>
             <li><span> [2023] Outstanding Overseas Student Scholarship - Government of China</li>
             <li><span> [2022] International Conference Student Support Award - University of Maryland</li>
             <li><span> [2022] CVPR Travel Grant </li>
             <li><span> [2022] Jacob K. Goldhaber Travel Grant - University of Maryland</li>
             <li><span> [2019] Teaching Assistant Training and Development (TATD) Fellow - University of Maryland</li>
             <li><span> [2015] Outstanding Graduates - Beijing Institute of Technology </li>
             <li><span> [2014] Honorable Mention - Mathematical Contest in Modeling </li>
        </ul>
   </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Experience</heading>
            </tr>
            <tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:20px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

                <td style="width:60px;vertical-align:middle">
                  <div class="inc-logo">
                    <img src='icons/umd.png' width="90">
                  </div>
                </td>

      <!--         <td style="width:60px;vertical-align:middle">
                <div class="inc-logo">
                  <img src='icons/umiacs.png' width="90">
                </div>
              </td> -->

                <td style="width:60px;vertical-align:middle">
                  <div class="inc-logo">
                    <img src='icons/umich.png' width="90">
                  </div>
                </td>

                <td style="width:60px;vertical-align:middle">
                  <div class="inc-logo">
                    <img src='icons/bit.png' width="90">
                  </div>
                </td>

              <td style="width:60px;vertical-align:middle">
                <div class="inc-logo">
                  <img src='icons/salesforce.png' width="90">
                </div>
              </td>

              </tr>
              <tr style="height: 40px"></tr>
              <tr style="border-top: 50px;">
              
                <td style="width:60px;vertical-align:middle">
                  <div class="inc-logo">
                    <img src='icons/Qualcomm.png' width="90">
                  </div>
                </td>

              <td style="width:60px;vertical-align:middle">
                <div class="inc-logo">
                  <img src='icons/apple.png' width="90">
                </div>
              </td>
              
              <td style="width:60px;vertical-align:middle">
                <div class="inc-logo">
                  <img src='icons/merl.png' width="90">
                </div>
              </td>

              <td style="width:60px;vertical-align:middle;">
                <div class="inc-logo">
                  <img src='icons/mediatek.png' width="90">
                </div>
              </td>
            </tr>
        </tbody></table>

        </tbody>
     </table>
  </table>
</body>

</html>
